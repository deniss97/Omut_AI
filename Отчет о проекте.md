### Отчет о проекте: Реализация метода WARP для улучшения языковой модели

#### Введение
Целью данного проекта было улучшение генерации текстов модели GPT2-imdb с использованием метода WARP (Weighted Approximate-Rank Pairwise) и сравнение результатов с базовой SFT (Supervised Fine-Tuning) моделью. Ожидалось, что метод WARP позволит модели более эффективно оценивать и классифицировать текстовые последовательности, что приведёт к улучшению качества генерируемых текстов.

#### Этапы проекта

1. Подготовка данных:
   - Были собраны и подготовлены данные для обучения моделей. Все данные были проверены на корректность и полноту.

2. Обучение базовой модели (SFT):
   - Была проведена настройка модели GPT2-imdb на основе собранных данных с использованием метода SFT. Результаты генерации текстов были зафиксированы для дальнейшего сравнения.

3. Реализация WARP метода:
   - Модель была обучена с использованием метода WARP для оценки влияния этого подхода на её способность генерировать текстовые продолжения.

4. Генерация и оценка текстов:
   - Тестирование моделей было проведено на одном и том же наборе запросов (prompts). Генерации текстовых продолжений обеих моделей были сравнены и оценены с использованием метрик Reward и KL Divergence.

#### Проблемы и наблюдения
Во время выполнения проекта были выявлены следующие проблемы:

1. Отсутствие различий в результатах генерации:
   - При сравнении результатов генераций текстов модели, обученной с использованием WARP, и базовой SFT модели, было обнаружено, что сгенерированные продолжения практически идентичны.
   - Все ключевые метрики (средняя награда и KL Divergence) оставались одинаковыми для обеих моделей. Это указывает на отсутствие значительного изменения в поведении модели при использовании WARP.

2. Проблемы с параметрами обучения:
   - Могут быть неправильно настроены или недостаточно значимо изменены гиперпараметры обучения, что привело к отсутствию различий между моделями.

3. Проблемы с процессом подготовки данных или кодом:
   - Возможны ошибки в процессе подготовки данных или в процессе кодирования модели, что привело к неизменности моделей.

#### Выводы
Реализация метода WARP для улучшения языковой модели GPT2-imdb не привела к ожидаемым результатам. Генерации текстов с обеих моделей оставались одинаковыми, несмотря на различные подходы к обучению. Это указывает на необходимость более глубокой проверки и отладки всего процесса обучения и тестирования.

#### Рекомендации
На основании выявленных проблем и наблюдений имеет смысл предпринять следующие шаги для дальнейшей работы:

1. Проверка и отладка кода:
   - Провести тщательную проверку кода на наличие ошибок или некорректных настроек, влияющих на процесс обучения моделей.
  
2. Анализ и оптимизация гиперпараметров:
   - Убедиться в том, что гиперпараметры обучения изменяются в достаточной мере для влияния на поведение модели. Возможны эксперименты с более широким диапазоном значений.

3. Использование дополнительных метрик и методов оценки:
   - Применить дополнительные методы оценки и логирования для более глубокого анализа поведения моделей и выявления потенциальных причин отсутствия различий.

4. Дополнительное обучение моделей:
   - Провести дополнительное обучение моделей с различными параметрами и подходами для выявления более точных причин отсутствия различий и их устранения.

#### Заключение
Несмотря на неудачу в реализации метода WARP, проект предоставил важные уроки и указал на ключевые области для улучшения и дальнейших исследований. Продолжение работы в этом направлении и исправление выявленных проблем могут привести к достигнутым целям и улучшению качества генерируемых текстов моделей GPT2-imdb.
